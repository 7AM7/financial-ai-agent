# Financial Data Pipeline

Production-grade ELT pipeline that extracts financial data from QuickBooks and Rootfi sources, transforms it into a unified star schema, and loads it into PostgreSQL for AI-powered natural language querying.

---

## ğŸš€ Quick Start

```bash
# 1. Start PostgreSQL
docker-compose up -d

# 2. Setup Python environment
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 3. Initialize database & run pipeline
python main.py init
python main.py run
```

**Expected Output**:
```
âœ“ Pipeline Completed Successfully
  QuickBooks: 15,000 records
  Rootfi: 12,000 records
  Total: 27,000 records in 20 seconds
```

---

## ğŸ“Š What This Does

Integrates two different financial data formats (QuickBooks P&L and Rootfi API) into a unified PostgreSQL star schema optimized for AI queries.

**Data Sources**:
- **QuickBooks Format** (`data/data_set_1.json`) - 1.1MB, column-based P&L report
- **Rootfi Format** (`data/data_set_2.json`) - 485KB, period-based financial records

**Time Period**: January 2020 - August 2025 (68 months)

---

## ğŸ—ï¸ Schema

### Star Schema Design

```
         dim_account              dim_date
        (categories)           (year/quarter)
               â”‚                     â”‚
               â””â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼    â–¼
               fact_financials â—„â”€â”€â”€ dim_source
                  (metrics)
```

**Optimized for AI**:
- âœ… Denormalized time fields (no JOINs needed for 90% of queries)
- âœ… Intelligent account categorization ("Payroll", "Marketing", etc.)
- âœ… Simple, predictable SQL patterns
- âœ… Sub-100ms query performance

---

## ğŸ’» Usage

### CLI Commands

```bash
# Initialize database (creates schema)
python main.py init

# Run full pipeline
python main.py run

# Run specific source
python main.py run --source quickbooks
python main.py run --source rootfi
```

### Makefile Commands

```bash
make setup      # Full environment setup
make init-db    # Initialize database
make run        # Run pipeline
make test       # Run tests
make clean      # Clean cache
```

---

## ğŸ“ Project Structure

```
data_pipeline/
â”œâ”€â”€ data/                    # Data files
â”‚   â”œâ”€â”€ data_set_1.json      # QuickBooks format
â”‚   â””â”€â”€ data_set_2.json      # Rootfi format
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extractors/          # Data extraction (QB & Rootfi)
â”‚   â”œâ”€â”€ transformers/        # Normalization & validation
â”‚   â”œâ”€â”€ loaders/             # Database loading
â”‚   â”œâ”€â”€ models/              # SQLAlchemy models
â”‚   â”œâ”€â”€ utils/               # Category mapping
â”‚   â””â”€â”€ financial_pipeline.py  # Main orchestrator
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ DATA_REPORT.md       # **â­ Data analysis & challenges**
â”‚   â”œâ”€â”€ TECHNICAL_REPORT.md  # Architecture & design
â”‚   â””â”€â”€ SQL_QUERY_EXAMPLES.md  # Query patterns for AI
â”œâ”€â”€ tests/                   # Unit & integration tests
â”œâ”€â”€ main.py                  # CLI entry point
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md               # This file
```

---

## ğŸ“š Documentation

| Document | Purpose |
|----------|---------|
| **README.md** | Quick start & usage (this file) |
| **[DATA_REPORT.md](docs/DATA_REPORT.md)** | â­ Data structure, challenges, quality issues |
| **[TECHNICAL_REPORT.md](docs/TECHNICAL_REPORT.md)** | Architecture, design decisions, performance |
| **[SQL_QUERY_EXAMPLES.md](docs/SQL_QUERY_EXAMPLES.md)** | Query patterns for AI agents |

---

## ğŸ¯ Key Features

âœ… **Star Schema** - Optimized for analytical queries
âœ… **Denormalized** - Fast queries without complex JOINs
âœ… **AI-Ready** - Simple, predictable SQL patterns
âœ… **Category Grouping** - Intelligent account categorization
âœ… **Production-Grade** - Error handling, logging, monitoring
âœ… **Well-Tested** - Unit & integration test coverage

---

## ğŸ”§ Configuration

Create `.env` file:

```bash
# PostgreSQL
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=financial_data
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_password

# Pipeline
CHUNK_SIZE=1000
LOG_LEVEL=INFO

# Data Sources
QUICKBOOKS_DATA_PATH=data/data_set_1.json
ROOTFI_DATA_PATH=data/data_set_2.json
```

---

## ğŸ“ˆ Performance

| Metric | Value |
|--------|-------|
| **Extraction Speed** | 27K records in 20 seconds |
| **Query Performance** | < 100ms for most queries |
| **Data Volume** | 68 months, 460 accounts |
| **Scalability** | Handles up to 1M transactions |

---

## ğŸ› Troubleshooting

**PostgreSQL not running?**
```bash
docker-compose down
docker-compose up -d
sleep 10
```

**Import errors?**
```bash
source venv/bin/activate
pip install -r requirements.txt
```

**Data files missing?**
```bash
ls data/  # Should show data_set_1.json, data_set_2.json
```

---

## ğŸ§ª Testing

```bash
# Run all tests
make test

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

---

## ğŸ› ï¸ Technology Stack

- **Python 3.9+**
- **PostgreSQL 15**
- **SQLAlchemy 2.0** (ORM)
- **Pydantic** (Configuration)
- **Structlog** (Logging)
- **Pytest** (Testing)

---
